{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùó No text from PdfReader. Trying OCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1731, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 894, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/js/r8dnglh16t30x5wv09386_w40000gp/T/ipykernel_91654/110115422.py\", line 88, in process_delegation_pdf\n",
      "    entries = extraction_chain.invoke({\"policy\": policy_text})\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 2879, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py\", line 5093, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 277, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 777, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 634, in generate\n",
      "    raise e\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 624, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 846, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 658, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 287, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 925, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/CS/Documents/DataExtraction/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from pypdf import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# === Load .env key ===\n",
    "_ = load_dotenv(find_dotenv())\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# === Schema ===\n",
    "class Delegation(BaseModel):\n",
    "    country: str = Field(description=\"Country name, like 'Ethiopia'\")\n",
    "    year: int = Field(description=\"Year of the UNGA session\")\n",
    "    officials: int = Field(description=\"Number of officials listed before 'Representatives'\")\n",
    "    leader_present: int = Field(description=\"1 if President or Prime Minister is listed among officials, otherwise 0\")\n",
    "    representatives: int = Field(description=\"Number of representatives\")\n",
    "    alternate_representatives: int = Field(description=\"Number of alternate representatives\")\n",
    "    advisers: int = Field(description=\"Number of advisers\")\n",
    "    attendees: int = Field(description=\"Total number of people across all above categories\")\n",
    "\n",
    "class DelegationInfo(BaseModel):\n",
    "    entries: List[Delegation]\n",
    "\n",
    "# === Prompt ===\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant that extracts delegation data from UN General Assembly reports.\"),\n",
    "    (\"human\", \"\"\"\n",
    "From the following text, extract the following for each country:\n",
    "- Country\n",
    "- Year\n",
    "- Number of officials (before 'Representatives')\n",
    "- Leader present (1 if President/Prime Minister is listed, 0 otherwise)\n",
    "- Representatives\n",
    "- Alternate representatives\n",
    "- Advisers\n",
    "- Total attendees\n",
    "\n",
    "Text: {policy}\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "# === LangChain Model ===\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "functions = [convert_to_openai_function(DelegationInfo)]\n",
    "extraction_model = model.bind(functions=functions, function_call={\"name\": \"DelegationInfo\"})\n",
    "extraction_chain = prompt_template | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"entries\")\n",
    "\n",
    "# === Text Extraction with OCR Fallback ===\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        raw_text = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
    "\n",
    "        if not raw_text.strip():\n",
    "            print(\"‚ùó No text from PdfReader. Trying OCR...\")\n",
    "            images = convert_from_path(file_path)\n",
    "            raw_text = \"\"\n",
    "            for img in images:\n",
    "                text = pytesseract.image_to_string(img)\n",
    "                raw_text += text\n",
    "\n",
    "        return raw_text\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {e}\"\n",
    "\n",
    "# === Main Gradio Function ===\n",
    "def process_delegation_pdf(pdf_file):\n",
    "    if pdf_file is None:\n",
    "        return \"‚ùå Please upload a file.\", None, pd.DataFrame()\n",
    "    \n",
    "    policy_text = extract_text_from_pdf(pdf_file.name)\n",
    "    if not policy_text.strip():\n",
    "        return \"‚ùå No text extracted from PDF.\", None, pd.DataFrame()\n",
    "\n",
    "    entries = extraction_chain.invoke({\"policy\": policy_text})\n",
    "    df = pd.DataFrame(entries)\n",
    "\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    output_path = os.path.join(\"outputs\", \"delegation_data.xlsx\")\n",
    "    df.to_excel(output_path, index=False)\n",
    "\n",
    "    return f\"‚úÖ Extracted {len(df)} entries\", output_path, df\n",
    "\n",
    "# === Gradio UI ===\n",
    "iface = gr.Interface(\n",
    "    fn=process_delegation_pdf,\n",
    "    inputs=gr.File(label=\"üìÑ Upload UNGA Report PDF\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Summary\"),\n",
    "        gr.File(label=\"‚¨áÔ∏è Excel File Output\"),\n",
    "        gr.DataFrame(label=\"üìä Extracted Table\")\n",
    "    ],\n",
    "    title=\"UN Delegation PDF Extractor\",\n",
    "    description=\"Upload a UNGA report PDF to extract delegation information and download an Excel file.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (564329780.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31muvicorn delegation_api:app --reload\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "uvicorn delegation_api:app --reload\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
